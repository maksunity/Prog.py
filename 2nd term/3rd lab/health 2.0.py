import os
import re
import codecs
import string
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
import umap.umap_ as umap


# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
def download_nltk_resources():
    try:
        nltk.data.find('corpora/wordnet')
        nltk.data.find('corpora/omw-1.4')
        stopwords.words('english')
    except LookupError:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK...")
        nltk.download(['stopwords', 'wordnet', 'omw-1.4'], quiet=True)


download_nltk_resources()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)


def preprocess_text(text):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    if not isinstance(text, str) or not text.strip():
        return ""

    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫, —Ü–∏—Ñ—Ä –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤
    text = re.sub(r"http\S+|\d+|[@#]", "", text.lower())

    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
    text = ''.join([c for c in text if c not in punctuation])
    text = re.sub(r"\s+", " ", text).strip()

    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
    tokens = text.split()
    return ' '.join([lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2])


# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
def load_data(folder):
    texts = []
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

    for filename in tqdm(os.listdir(folder)):
        if filename.endswith(".txt"):
            try:
                with codecs.open(os.path.join(folder, filename), 'r',
                                 encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
            except Exception as e:
                print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")
                continue

            for line in lines:
                try:
                    parts = line.strip().split("|")
                    if len(parts) >= 3:
                        processed = preprocess_text(parts[2])
                        if processed:
                            texts.append(processed)
                except Exception as e:
                    print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏: {line[:50]}...: {e}")
                    continue

    print(f"\n–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")
    return texts


# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥
if __name__ == "__main__":
    folder = "Health-Tweets"
    if not os.path.exists(folder):
        raise FileNotFoundError(f"–ü–∞–ø–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ '{folder}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

    texts = load_data(folder)

    if not texts:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞!")

    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 2),
        max_features=5000,
        min_df=3,
        max_df=0.7
    )
    X_tfidf = vectorizer.fit_transform(texts)

    # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    print("\n–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
    inertia = []
    silhouettes = []
    k_range = range(3, 11)

    for k in tqdm(k_range):
        try:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_tfidf)
            inertia.append(kmeans.inertia_)
            silhouettes.append(silhouette_score(X_tfidf, labels))
        except Exception as e:
            print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è k={k}: {e}")
            continue

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(k_range, inertia, 'bo-')
    plt.title('–ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è')
    plt.xlabel('–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')

    plt.subplot(1, 2, 2)
    plt.plot(k_range, silhouettes, 'ro-')
    plt.title('–°–∏–ª—É—ç—Ç-–∞–Ω–∞–ª–∏–∑')
    plt.xlabel('–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
    plt.tight_layout()
    plt.show()

    # –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k
    optimal_k = k_range[np.argmax(silhouettes)]
    print(f"\n–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_k}")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_tfidf)

    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    print(f"\n–°–∏–ª—É—ç—Ç-–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: {silhouette_score(X_tfidf, labels):.2f}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å UMAP
    print("\n–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
    try:
        reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1)
        X_umap = reducer.fit_transform(X_tfidf.toarray())

        plt.figure(figsize=(10, 6))
        scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap="Spectral", s=5)
        plt.title(f"2D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è {optimal_k} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (UMAP)")
        plt.colorbar(scatter)
        plt.show()
    except Exception as e:
        print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ UMAP: {e}")
        print("–ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TSNE –≤–º–µ—Å—Ç–æ UMAP...")

        try:
            tsne = TSNE(n_components=2, random_state=42)
            X_tsne = tsne.fit_transform(X_tfidf.toarray())

            plt.figure(figsize=(10, 6))
            scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap="Spectral", s=5)
            plt.title(f"2D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è {optimal_k} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (TSNE)")
            plt.colorbar(scatter)
            plt.show()
        except Exception as e:
            print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ TSNE: {e}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n–¢–æ–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()

    for i in range(kmeans.n_clusters):
        top_terms = [terms[ind] for ind in order_centroids[i, :10]]
        print(f"üîπ –ö–ª–∞—Å—Ç–µ—Ä {i}: {', '.join(top_terms)}")

    # –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    print("\n–ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    for cluster_id in range(kmeans.n_clusters):
        print(f"\n–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}")
        cluster_texts = [text for idx, text in enumerate(texts) if labels[idx] == cluster_id]
        for example in cluster_texts[:3]:
            print(f"  - {example}")