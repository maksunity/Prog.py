Практика 2
Разработать систему сбора данных с современных веб-сайтов, которые требуют различных подходов к извлечению информации:

1. создать Scrapy-проект для сбора структурированных данных с сайтов с классической архитектурой
2. интегрировать Playwright в Scrapy для обработки динамического контента
3. реализовать минимум 2 различных паука: для новостного портала с SSR, для динамического сайта, например, e-commerce
4. каждый паук должен собрать не менее 100 записей
5. необходимо реализовать также обработку ошибок и повторные попытки
Должна быть поддержка:
статических сайтов (через классические Scrapy Spider)
динамические сайты и SPA-приложения (через Playwright)
Попробовать еще добавить сайты с анти-бот защитой (через эмуляцию пользовательского поведения), например, через сервисы типа Anti-Captcha

Что использовать:
Scrapy — фреймворк для масштабируемого веб-скрапинга
Playwright — инструмент для автоматизации современных браузеров
Scrapy-Playwright — интеграция двух технологий
Опционально Redis для распределенной очереди задач и Docker для управления зависимостями

Выгрузку сделать в JSON и CSV
Пример запуска:
python ./run_spider.py --spider news --keywords "ПНИПУ"--days 7--output
news_pstu.json
python ./run_spider.py --spider ecommerce --category "ноутбуки"--stores
"wildberries,ozon"--output prices.cs